# K-Means-Clustering

#### What is K-Means Clustering?
When dealing with large amounts of data, K-Means clustering is a useful unsupervised learning algorithm that groups data into clusters based on their similarities. 
Here, 'K'represents the number of clusters that is required before the clustering process begins.
Note : It is important to determine the optimal number of clusters before initiating the clustering process. 

This is where techniques like the **'Elbow method'** or **'Silhouette method'** come in handy. These methods allow us to identify the best value for 'K', which is essential for accurate and efficient clustering. By using these methods, we can simplify the data analysis process and achieve superior results.

#### Why Do we use K-Means Clustering?
- Based on my research, I have discovered that K-Means clustering is a highly effective algorithm for handling datasets that lack labels. This clustering technique is particularly useful when it comes to visualizing data both before and after it has been preprocessed.
-  One of the major advantages of K-Means clustering is its ability to quickly and accurately identify anomalies within clusters. Another major benefit is that it is highly efficient for larger datasets, as it groups clusters based on their similarities, which helps to reduce redundancy and improve overall performance. -
-  Overall, K-Means clustering is a valuable tool for anyone working with unlabelled datasets, as it can help to uncover hidden patterns and insights that might otherwise go unnoticed.
-  Since K-Means clustering is an unsupervised algorithm, it is ideal for datasets with unlabelled data.
-  Helps with better data visualisation, both before and after data has been preprocessed
-  Quickly identifies anomalies within clusters.
-  It is efficient for large datasets as it groups clusters based on their similarities, reducing redundancy.


## Task A : Iris Data
Perform K-Means clustering on the dataset, iris.csv (from the UCI Machine Learning Repository). Before using the data for clustering, you might have to remove a few columns because the K-Means algorithm involves the calculation of Euclidian distance. You can choose various values of K; however, you must also choose K = 3 in this case. Upon clustering at K = 3, check how much similar your three clusters are as compared to the labels of species – setosa, versicolour, and verginica.

### Insights To the Iris Dataset


## Task B: Wine Data
Perform K-Means clustering on the dataset, wine.csv (from the UCI Machine Learning Repository). Before using the data for clustering, you might have to remove a few columns like in the previous task. You can choose various values of K; however, you must also choose K = 3 in this case. Upon clustering at K = 3, check how much similar your three clusters are as compared to the labels of wines – 1, 2, and 3.

### Insights To the Wine Dataset


## Task C: WeatherAUS Data
Perform K-Means clustering on the dataset, weatherAUS.csv (from the Kaggle site). The details about this data can be found here. Again, you will have to remove a few columns from this data and make it useful for clustering. Play with a range of values of K, from, K=2 to K=6 and try to visualise the results of clustering using two-dimensional scatter plots.

### Insights To the WeatherAUS Dataset
